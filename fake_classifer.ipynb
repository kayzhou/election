{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: Kay Zhou\n",
    "# Date: 2019-02-24 16:42:55\n",
    "\n",
    "from itertools import chain\n",
    "import gc\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "import SQLite_handler\n",
    "from my_weapon import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from myclf import *\n",
    "from Trump_Clinton_Classifer.TwProcess import CustomTweetTokenizer\n",
    "from Trump_Clinton_Classifer.TwSentiment import bag_of_words, bag_of_words_and_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fake_Classifer(object):\n",
    "    def __init__(self):\n",
    "        self.MAP_LABELS = {\n",
    "            \"0\": \"fake\",\n",
    "            \"1\": \"extreme bias (right)\",\n",
    "            \"2\": \"right\",\n",
    "            \"3\": \"right leaning\",\n",
    "            \"4\": \"center\",\n",
    "            \"5\": \"left leaning\",\n",
    "            \"6\": \"left\",\n",
    "            \"7\": \"extreme bias (left)\"\n",
    "        }\n",
    "\n",
    "    def get_train_data(self):\n",
    "        \"\"\"\n",
    "        获取训练文本\n",
    "        \"\"\"\n",
    "        print(\"loading all tweets_csv ...\")\n",
    "        all_tweets = pd.read_csv(\"disk/all-tweets.csv\", dtype=str, usecols=[\"tweet_id\", \"media_type\"])\n",
    "        print(\"finished!\")\n",
    "\n",
    "        map_labels = {\n",
    "            \"0\": \"fake\",\n",
    "            \"1\": \"extreme bias (right)\",\n",
    "            \"2\": \"right\",\n",
    "            \"3\": \"right leaning\",\n",
    "            \"4\": \"center\",\n",
    "            \"5\": \"left leaning\",\n",
    "            \"6\": \"left\",\n",
    "            \"7\": \"extreme bias (left)\"\n",
    "        }\n",
    "\n",
    "        for _type, f_label in map_labels.items():\n",
    "            print(_type, \"...\")\n",
    "            tweets_id = all_tweets[all_tweets[\"media_type\"] == _type].tweet_id\n",
    "            rst = SQLite_handler.find_tweets(tweets_id)\n",
    "            print(len(rst))\n",
    "            with open(\"disk/train_data_fake/{}.txt\".format(_type), \"w\") as f:\n",
    "                for d in rst:\n",
    "                    if \"text\" not in d:\n",
    "                        continue\n",
    "                    # elif d[\"text\"].startswith(\"RT\"):\n",
    "                    #     continue\n",
    "                    f.write(d[\"text\"] + \"\\n\")\n",
    "\n",
    "    def get_tokens(self):\n",
    "        \"\"\"\n",
    "        text > tokens\n",
    "        \"\"\"\n",
    "        tokenizer = CustomTweetTokenizer()\n",
    "\n",
    "        for _type, f_label in self.MAP_LABELS.items():\n",
    "            with open(\"disk/tokens_fake/{}.txt\".format(_type), \"w\") as f:\n",
    "                for line in open(\"disk/train_data_fake/{}.txt\".format(_type)):\n",
    "                    words = tokenizer.tokenize(line.strip())\n",
    "                    if len(words) > 0 and words[0] != \"RT\":\n",
    "                        f.write(\" \".join(words) + \"\\n\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        fake, non-fake\n",
    "        fake, left, center, right √ 优先\n",
    "        left, center, right\n",
    "        \"\"\"\n",
    "        # read data\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for _type, f_label in self.MAP_LABELS.items():\n",
    "\n",
    "            if f_label == \"fake\":\n",
    "                y_i = 0 \n",
    "            elif f_label in [\"extreme bias (right)\", \"right\", \"right leaning\"]:\n",
    "                y_i = 1 \n",
    "            elif f_label == \"center\":\n",
    "                y_i = 2 \n",
    "            elif f_label in [\"extreme bias (left)\", \"left\", \"left leaning\"]:\n",
    "                y_i = 3\n",
    "\n",
    "            for i, line in enumerate(open(\"disk/tokens_fake/{}.txt\".format(_type))):\n",
    "                w = line.strip().split(\" \")\n",
    "                # if len(w) > 0 and w[0] != \"RT\":\n",
    "                X.append(bag_of_words_and_bigrams(w))\n",
    "                # print(X[-1])\n",
    "                y.append(y_i)\n",
    "\n",
    "        print(\"Reading data finished! count:\", len(y))\n",
    "\n",
    "        # split train and test data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        \n",
    "        del X, y\n",
    "        gc.collect()\n",
    "        print(\"Splitting data finished!\")\n",
    "\n",
    "        # build one hot embedding\n",
    "        v = DictVectorizer(dtype=np.int8, sparse=True, sort=False)\n",
    "        X_train = v.fit_transform(X_train)\n",
    "        X_test = v.transform(X_test)\n",
    "\n",
    "        print(\"Building word embedding finished!\")\n",
    "        print(X_train[0].shape, X_train[1].shape)\n",
    "        print(X_train.shape, X_test.shape)\n",
    "\n",
    "        # machine learning model\n",
    "        # list_classifiers = ['LR', 'GBDT', 'NB', 'RF']\n",
    "        list_classifiers = ['NB', 'LR', 'SVMLINER', 'GBDT']\n",
    "        classifiers = {\n",
    "            'NB': naive_bayes_classifier,\n",
    "            'KNN': knn_classifier,\n",
    "            'LR': logistic_regression_classifier,\n",
    "            'RF': random_forest_classifier,\n",
    "            'DT': decision_tree_classifier,\n",
    "            'SVM': svm_classifier,\n",
    "            'SVMCV': svm_cross_validation,\n",
    "            'GBDT': gradient_boosting_classifier,\n",
    "            'SVMLINER': svm_linear_classifier,\n",
    "        }\n",
    "\n",
    "        for classifier in list_classifiers:\n",
    "            print('******************* {} ********************'.format(classifier))\n",
    "            if classifier == \"GBDT\":\n",
    "                clf = GradientBoostingClassifier(learning_rate=0.1, max_depth=3)\n",
    "                clf.fit(X_train, y_train)\n",
    "            if classifier == \"LR\":\n",
    "                clf = LogisticRegression(penalty='l2', multi_class=\"multinomial\", solver=\"sag\", max_iter=10e8)\n",
    "                clf.fit(X_train, y_train)\n",
    "            else:\n",
    "                clf = classifiers[classifier](X_train, y_train)\n",
    "            # print(\"fitting finished! Lets evaluate!\")\n",
    "            self.evaluate(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "        # original_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': 3, 'random_state': 23,\n",
    "        #                 'min_samples_split': 5}\n",
    "\n",
    "        # for GDBT\n",
    "        # for i, setting in enumerate([{'learning_rate': 1.0, 'subsample': 1.0},\n",
    "        #                 {'learning_rate': 0.1, 'subsample': 1.0},\n",
    "        #                 {'learning_rate': 1.0, 'subsample': 0.5},\n",
    "        #                 {'learning_rate': 0.1, 'subsample': 0.5},\n",
    "        #                 {'learning_rate': 0.1, 'max_features': 2}]):\n",
    "        #     print('******************* {} ********************'.format(i))\n",
    "        #     params = dict(original_params)\n",
    "        #     params.update(setting)\n",
    "\n",
    "        #     clf = GradientBoostingClassifier(**params)\n",
    "        #     clf.fit(X_train, y_train)\n",
    "        #     self.evaluate(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "        # original_params = {}\n",
    "\n",
    "        # LinearSVC\n",
    "        # for i, setting in enumerate([{'C':0.125}, {'C': 0.25}, {'C':0.5}, {'C':1.0}, {'C':2.0}, {'C': 4.0}, {'C':8.0}]):\n",
    "        #     print('******************* {} ********************'.format(i))\n",
    "        #     print(setting)\n",
    "        #     params = dict(original_params)\n",
    "        #     params.update(setting)\n",
    "\n",
    "        #     clf = LinearSVC(**params)\n",
    "        #     clf.fit(X_train, y_train)\n",
    "        #     self.evaluate(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "    def evaluate(self, clf, X_train, y_train, X_test, y_test):\n",
    "        # CV\n",
    "        print('accuracy of CV=5:', cross_val_score(clf, X_train, y_train, cv=5).mean())\n",
    "\n",
    "        # 模型评估\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data finished! count: 17493472\n",
      "Splitting data finished!\n",
      "Building word embedding finished!\n",
      "(1, 6483620) (1, 6483620)\n",
      "(15744124, 6483620) (1749348, 6483620)\n",
      "******************* NB ********************\n",
      "accuracy of CV=5: 0.7363921930798532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74    186229\n",
      "           1       0.76      0.70      0.73    578247\n",
      "           2       0.80      0.65      0.72    323829\n",
      "           3       0.68      0.85      0.76    661043\n",
      "\n",
      "   micro avg       0.74      0.74      0.74   1749348\n",
      "   macro avg       0.78      0.71      0.74   1749348\n",
      "weighted avg       0.75      0.74      0.74   1749348\n",
      "\n",
      "******************* LR ********************\n"
     ]
    }
   ],
   "source": [
    "Lebron = Fake_Classifer()\n",
    "# Lebron.get_train_data()\n",
    "# Lebron.get_tokens()\n",
    "Lebron.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
