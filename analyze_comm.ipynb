{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_weapon import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from fake_identify import Are_you_IRA\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "matplotlib.rcParams[\"font.size\"] = 14\n",
    "sns.set_style(\"darkgrid\")\n",
    "ira_c = sns.color_palette(\"coolwarm\", 8)[7]\n",
    "all_c = sns.color_palette(\"coolwarm\", 8)[0]\n",
    "\n",
    "Putin = Are_you_IRA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\n",
    "    \"rt\", \"…\", \"...\", \"URL\", \"http\", \"https\", \"“\", \"”\", \"‘\", \"’\", \"get\", \"2\", \"new\", \"one\", \"i'm\", \"make\",\n",
    "    \"go\", \"good\", \"say\", \"says\", \"know\", \"day\", \"..\", \"take\", \"got\", \"1\", \"going\", \"4\", \"3\", \"two\", \"n\",\n",
    "    \"like\", \"via\", \"u\", \"would\", \"still\", \"first\", \"really\", \"watch\", \"see\", \"even\", \"that's\", \"look\", \"way\",\n",
    "    \"last\", \"said\", \"let\", \"twitter\", \"ever\", \"always\", \"another\", \"many\", \"things\", \"may\", \"big\", \"come\", \"keep\",\n",
    "    \"5\", \"time\", \"much\", \"want\", \"think\", \"us\", \"love\", \"people\", \"need\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trump_Clinton_Classifer.TwProcess import CustomTweetTokenizer\n",
    "\n",
    "tokenizer = CustomTweetTokenizer(preserve_case=False,\n",
    "                                 reduce_len=True,\n",
    "                                 strip_handles=False,\n",
    "                                 normalize_usernames=False,\n",
    "                                 normalize_urls=True,\n",
    "                                 keep_allupper=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd68b12b6fc420cacea2d87263634a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "391680\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "data = pd.read_csv(\"data/ira-tweets-ele.csv\", usecols=[\"tweet_text\"])\n",
    "for i, row in tqdm(data.iterrows()):\n",
    "    # if row[\"userid\"] not in users_comm:\n",
    "    #     continue\n",
    "    words = tokenizer.tokenize(row[\"tweet_text\"])\n",
    "    words = [w for w in words if w not in stop_words and w]\n",
    "    # if words[0] == \"RT\":\n",
    "    #     continue\n",
    "#     for w in words:\n",
    "#         cnt[w] += 1\n",
    "    texts.append(words)\n",
    "print(len(texts))\n",
    "# json.dump(cnt.most_common(), open(\"data/word_cloud.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# nlp = spacy.load('en')\n",
    "import re\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    i = 0\n",
    "    for sent in tqdm(texts):\n",
    "        sent = \" \".join(sent)\n",
    "        sent = re.sub(r'#(\\w+)', r'itstopiczzz\\1', sent)\n",
    "        sent = re.sub(r'@(\\w+)', r'itsmentionzzz\\1', sent)\n",
    "        doc = nlp(sent)\n",
    "        \n",
    "        _d = [token.lemma_ for token in doc if token.pos_ in allowed_postags and token.lemma_ not in stop_words]\n",
    "#         _d = [(token.pos_, token.lemma_) for token in doc if token.lemma_ not in stop_words]\n",
    "        \n",
    "        _d = [x.replace('itstopiczzz', '#') for x in _d]\n",
    "        _d = [x.replace('itsmentionzzz', '@') for x in _d]\n",
    "        texts_out.append(_d)\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55885be36a64fbcb632a420725d0b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391680), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_out = lemmatization(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391680"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(texts_out)\n",
    "\n",
    "# Create Corpus\n",
    "texts = texts_out\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
